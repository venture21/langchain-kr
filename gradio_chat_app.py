import gradio as gr
from operator import itemgetter
from langchain.memory import (
    ConversationBufferMemory,
    ConversationSummaryMemory,
    ConversationBufferWindowMemory,
    ConversationTokenBufferMemory,
    ConversationEntityMemory,
    ConversationKGMemory,
    VectorStoreRetrieverMemory,
)
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv
import faiss
from langchain.docstore import InMemoryDocstore
from langchain.vectorstores import FAISS

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class MyConversationChain(Runnable):
    def __init__(self, llm, prompt, memory, input_key="input"):
        self.prompt = prompt
        self.memory = memory
        self.input_key = input_key

        # ë©”ëª¨ë¦¬ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ memory_key ì„¤ì •
        memory_key = self.get_memory_key(memory)

        if isinstance(memory, VectorStoreRetrieverMemory):
            # VectorStoreRetrieverMemoryì˜ ê²½ìš° ê²€ìƒ‰ëœ ê´€ë ¨ ëŒ€í™”ë¥¼ ì‚¬ìš©
            def get_vector_history(inputs):
                try:
                    memory_vars = self.memory.load_memory_variables(inputs)
                    history_text = memory_vars.get("history", "")
                    if history_text:
                        # ë¬¸ìì—´ì„ ê°„ë‹¨í•œ ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜
                        from langchain_core.messages import HumanMessage, AIMessage

                        messages = []
                        # ê°„ë‹¨í•œ íŒŒì‹±ìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ)
                        if "Human:" in history_text and "AI:" in history_text:
                            parts = history_text.split("\n")
                            current_human = ""
                            current_ai = ""
                            for part in parts:
                                if part.startswith("Human:"):
                                    if current_human and current_ai:
                                        messages.extend(
                                            [
                                                HumanMessage(content=current_human),
                                                AIMessage(content=current_ai),
                                            ]
                                        )
                                    current_human = part.replace("Human:", "").strip()
                                    current_ai = ""
                                elif part.startswith("AI:"):
                                    current_ai = part.replace("AI:", "").strip()
                                elif current_ai:
                                    current_ai += " " + part.strip()
                                elif current_human:
                                    current_human += " " + part.strip()
                            if current_human and current_ai:
                                messages.extend(
                                    [
                                        HumanMessage(content=current_human),
                                        AIMessage(content=current_ai),
                                    ]
                                )
                        return messages
                    return []
                except:
                    return []

            self.chain = (
                RunnablePassthrough.assign(
                    chat_history=RunnableLambda(get_vector_history)
                )
                | prompt
                | llm
                | StrOutputParser()
            )
        else:
            self.chain = (
                RunnablePassthrough.assign(
                    chat_history=RunnableLambda(self.memory.load_memory_variables)
                    | itemgetter(memory_key)
                )
                | prompt
                | llm
                | StrOutputParser()
            )

    def get_memory_key(self, memory):
        """ë©”ëª¨ë¦¬ íƒ€ì…ì— ë”°ë¼ ì˜¬ë°”ë¥¸ memory_key ë°˜í™˜"""
        if isinstance(memory, VectorStoreRetrieverMemory):
            return "history"
        elif hasattr(memory, "memory_key"):
            return memory.memory_key
        elif hasattr(memory, "chat_memory_key"):
            return memory.chat_memory_key
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ 'history' ì‚¬ìš© (ëŒ€ë¶€ë¶„ì˜ ë©”ëª¨ë¦¬ íƒ€ì…ì—ì„œ ì‚¬ìš©)
            return "history"

    def invoke(self, query, configs=None, **kwargs):
        answer = self.chain.invoke({self.input_key: query})
        self.memory.save_context(inputs={"human": query}, outputs={"ai": answer})
        return answer

    def clear_memory(self):
        self.memory.clear()


class GradioChatApp:
    def __init__(
        self,
        model_name="gpt-4o",
        temperature=0.0,
        memory_type="ConversationBufferMemory",
        memory_params=None,
    ):
        self.model_name = model_name
        self.temperature = temperature
        self.memory_type = memory_type
        self.memory_params = memory_params or {}
        self.conversation_chain = None
        self.initialize_chain()

    def initialize_chain(self):
        # ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
        llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)

        # ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a helpful and friendly AI assistant. Respond in Korean unless otherwise requested.",
                ),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ]
        )

        # ì„ íƒí•œ ë©”ëª¨ë¦¬ íƒ€ì…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìƒì„±
        memory = self.create_memory(self.memory_type, **self.memory_params)

        # ëŒ€í™” ì²´ì¸ ìƒì„±
        self.conversation_chain = MyConversationChain(llm, prompt, memory)

    def create_memory(self, memory_type, **params):
        """ì„ íƒí•œ ë©”ëª¨ë¦¬ íƒ€ì…ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìƒì„±"""
        if memory_type == "ConversationBufferMemory":
            return ConversationBufferMemory(
                return_messages=True, memory_key="chat_history"
            )

        elif memory_type == "ConversationBufferWindowMemory":
            k = params.get("k", 2)
            return ConversationBufferWindowMemory(
                k=k, return_messages=True, memory_key="chat_history"
            )

        elif memory_type == "ConversationTokenBufferMemory":
            llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)
            max_token_limit = params.get("max_token_limit", 100)
            return ConversationTokenBufferMemory(
                llm=llm,
                max_token_limit=max_token_limit,
                return_messages=True,
                memory_key="chat_history",
            )

        elif memory_type == "ConversationEntityMemory":
            llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)
            # ConversationEntityMemoryëŠ” memory_key íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            try:
                return ConversationEntityMemory(
                    llm=llm, return_messages=True, memory_key="chat_history"
                )
            except TypeError:
                return ConversationEntityMemory(llm=llm, return_messages=True)

        elif memory_type == "ConversationKnowledgeGraph":
            llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)
            # ConversationKGMemoryëŠ” memory_key íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            try:
                return ConversationKGMemory(
                    llm=llm, return_messages=True, memory_key="chat_history"
                )
            except TypeError:
                return ConversationKGMemory(llm=llm, return_messages=True)

        elif memory_type == "ConversationSummary":
            llm = ChatOpenAI(model_name=self.model_name, temperature=self.temperature)
            max_token_limit = params.get("max_token_limit", 100)
            return ConversationSummaryMemory(
                llm=llm,
                max_token_limit=max_token_limit,
                return_messages=True,
                memory_key="chat_history",
            )

        elif memory_type == "VectorStoreRetrieverMemory":
            # ì„ë² ë”© ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.
            embeddings_model = OpenAIEmbeddings()

            # Vector Store ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.
            embedding_size = 1536
            index = faiss.IndexFlatL2(embedding_size)
            vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})

            # ë²¡í„° ì¡°íšŒê°€ ì—¬ì „íˆ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ì •ë³´ë¥¼ ë°˜í™˜í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.
            k = params.get("k", 1)
            retriever = vectorstore.as_retriever(search_kwargs={"k": k})
            return VectorStoreRetrieverMemory(retriever=retriever)

        else:
            # ê¸°ë³¸ê°’
            return ConversationBufferWindowMemory(
                k=2, return_messages=True, memory_key="chat_history"
            )

    def get_vector_search_result(self, query):
        """VectorStoreRetrieverMemoryì—ì„œ ìœ ì‚¬í•œ ëŒ€í™” ê²€ìƒ‰"""
        if self.memory_type == "VectorStoreRetrieverMemory":
            try:
                # ë©”ëª¨ë¦¬ì—ì„œ ê´€ë ¨ ëŒ€í™” ê²€ìƒ‰
                memory_vars = self.conversation_chain.memory.load_memory_variables(
                    {"input": query}
                )
                history_text = memory_vars.get("history", "")
                if history_text:
                    return history_text
                else:
                    return "ë°ì´í„° ì—†ìŒ"
            except Exception as e:
                return f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"
        return ""

    def respond(self, message, history):
        """Gradio ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ„í•œ ì‘ë‹µ í•¨ìˆ˜"""
        try:
            # LangChain ëŒ€í™” ì²´ì¸ì„ í†µí•´ ì‘ë‹µ ìƒì„±
            response = self.conversation_chain.invoke(message)
            return response
        except Exception as e:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        if self.conversation_chain:
            self.conversation_chain.clear_memory()
        return None, None

    def update_settings(self, model_name, temperature, memory_type, memory_params):
        """ëª¨ë¸ ë° ë©”ëª¨ë¦¬ ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.model_name = model_name
        self.temperature = temperature
        self.memory_type = memory_type
        self.memory_params = memory_params
        self.initialize_chain()
        return f"ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ íƒ€ì…: {memory_type}"


def create_gradio_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    app = GradioChatApp()

    with gr.Blocks(title="LangChain Gradio Chat", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ¤– LangChain Gradio Chat
            LangChainê³¼ OpenAIë¥¼ í™œìš©í•œ ëŒ€í™”í˜• AI ì±—ë´‡ì…ë‹ˆë‹¤.
            """
        )

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    label="ëŒ€í™”",
                    height=500,
                    bubble_full_width=False,
                    avatar_images=(None, "ğŸ¤–"),
                )
                msg = gr.Textbox(
                    label="ë©”ì‹œì§€ ì…ë ¥",
                    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (Enter í‚¤ë¡œ ì „ì†¡)",
                    lines=2,
                )

                # VectorStoreRetrieverMemory ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ ì°½
                with gr.Group() as vector_search_group:
                    vector_search_output = gr.Textbox(
                        label="ğŸ” ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ (ìœ ì‚¬í•œ ëŒ€í™” ê¸°ë¡)",
                        placeholder="VectorStoreRetrieverMemory ì‚¬ìš© ì‹œ ê²€ìƒ‰ëœ ê´€ë ¨ ëŒ€í™”ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.",
                        lines=5,
                        interactive=False,
                        visible=False,
                    )

                with gr.Row():
                    submit = gr.Button("ì „ì†¡", variant="primary")
                    clear = gr.Button("ëŒ€í™” ì´ˆê¸°í™”")

            with gr.Column(scale=1):
                gr.Markdown("### âš™ï¸ ì„¤ì •")

                # ëª¨ë¸ ì„¤ì •
                model_dropdown = gr.Dropdown(
                    choices=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
                    value="gpt-4o-mini",
                    label="ëª¨ë¸ ì„ íƒ",
                )
                temperature_slider = gr.Slider(
                    minimum=0,
                    maximum=2,
                    value=0.7,
                    step=0.1,
                    label="Temperature (ì°½ì˜ì„± ìˆ˜ì¤€)",
                )

                # ë©”ëª¨ë¦¬ ì„¤ì •
                gr.Markdown("#### ğŸ’­ ë©”ëª¨ë¦¬ ì„¤ì •")
                memory_dropdown = gr.Dropdown(
                    choices=[
                        "ConversationBufferMemory",
                        "ConversationBufferWindowMemory",
                        "ConversationTokenBufferMemory",
                        "ConversationEntityMemory",
                        "ConversationKnowledgeGraph",
                        "ConversationSummary",
                        "VectorStoreRetrieverMemory",
                    ],
                    value="ConversationBufferWindowMemory",
                    label="ë©”ëª¨ë¦¬ íƒ€ì…",
                    info="ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”",
                )

                # ë©”ëª¨ë¦¬ë³„ ì„¤ì • íŒŒë¼ë¯¸í„°
                with gr.Group(visible=True) as window_params:
                    k_slider = gr.Slider(
                        minimum=1,
                        maximum=20,
                        value=2,
                        step=1,
                        label="Window Size (k) - ê¸°ì–µí•  ëŒ€í™” í„´ ìˆ˜",
                    )

                with gr.Group(visible=False) as token_params:
                    token_slider = gr.Slider(
                        minimum=50,
                        maximum=1000,
                        value=100,
                        step=50,
                        label="Max Token Limit - ìµœëŒ€ í† í° ìˆ˜",
                    )

                with gr.Group(visible=False) as summary_params:
                    summary_token_slider = gr.Slider(
                        minimum=50,
                        maximum=500,
                        value=100,
                        step=25,
                        label="Max Token Limit - ìš”ì•½ ìƒì„± ê¸°ì¤€ í† í° ìˆ˜",
                    )

                with gr.Group(visible=False) as vector_params:
                    vector_k_slider = gr.Slider(
                        minimum=1,
                        maximum=10,
                        value=1,
                        step=1,
                        label="Vector Search K - ê²€ìƒ‰í•  ë²¡í„° ê°œìˆ˜",
                    )

                update_btn = gr.Button("ì„¤ì • ì ìš©", variant="secondary")
                settings_output = gr.Textbox(label="ì„¤ì • ìƒíƒœ", interactive=False)

                gr.Markdown(
                    """
                    ### ğŸ“ ì‚¬ìš© ë°©ë²•
                    1. ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enter ë˜ëŠ” ì „ì†¡ ë²„íŠ¼ì„ í´ë¦­
                    2. AIê°€ ì„ íƒí•œ ë©”ëª¨ë¦¬ ë°©ì‹ìœ¼ë¡œ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µ
                    3. ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ 'ëŒ€í™” ì´ˆê¸°í™”' í´ë¦­
                    4. ì„¤ì • ë³€ê²½ ì‹œ 'ì„¤ì • ì ìš©' í´ë¦­
                    
                    ### ğŸ’­ ë©”ëª¨ë¦¬ íƒ€ì… ì„¤ëª…
                    - **Buffer**: ëª¨ë“  ëŒ€í™” ê¸°ë¡ ì €ì¥
                    - **BufferWindow**: ìµœê·¼ kê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
                    - **TokenBuffer**: í† í° ìˆ˜ ì œí•œìœ¼ë¡œ ê¸°ë¡ ê´€ë¦¬
                    - **Entity**: ì¤‘ìš” ê°œì²´ ì •ë³´ ì¶”ì¶œí•˜ì—¬ ê¸°ì–µ
                    - **KnowledgeGraph**: ì§€ì‹ ê·¸ë˜í”„ë¡œ ê´€ê³„ ì €ì¥
                    - **Summary**: ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì—¬ ì €ì¥
                    - **VectorStoreRetriever**: ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ëŒ€í™” ê¸°ì–µ
                    """
                )

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²°
        def user_submit(user_message, history):
            if not user_message:
                return "", history
            history = history or []
            return "", history + [[user_message, None]]

        def bot_respond(history):
            if history and history[-1][1] is None:
                user_message = history[-1][0]

                # VectorStoreRetrieverMemory ì‚¬ìš© ì‹œ ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                vector_search_result = ""
                if app.memory_type == "VectorStoreRetrieverMemory":
                    vector_search_result = app.get_vector_search_result(user_message)

                bot_message = app.respond(user_message, history[:-1])
                history[-1][1] = bot_message

                return history, vector_search_result
            return history, ""

        def update_memory_ui(memory_type):
            """ë©”ëª¨ë¦¬ íƒ€ì…ì— ë”°ë¼ ì„¤ì • UI í‘œì‹œ/ìˆ¨ê¹€"""
            vector_search_visible = memory_type == "VectorStoreRetrieverMemory"

            if memory_type == "ConversationBufferWindowMemory":
                return (
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=vector_search_visible),
                )
            elif memory_type == "ConversationTokenBufferMemory":
                return (
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=vector_search_visible),
                )
            elif memory_type == "ConversationSummary":
                return (
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=False),
                    gr.update(visible=vector_search_visible),
                )
            elif memory_type == "VectorStoreRetrieverMemory":
                return (
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=True),
                    gr.update(visible=vector_search_visible),
                )
            else:
                return (
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=vector_search_visible),
                )

        def apply_settings(
            model_name,
            temperature,
            memory_type,
            k_value,
            token_limit,
            summary_token_limit,
            vector_k,
        ):
            """ì„¤ì • ì ìš©"""
            # ë©”ëª¨ë¦¬ íƒ€ì…ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„° ì„¤ì •
            memory_params = {}
            if memory_type == "ConversationBufferWindowMemory":
                memory_params["k"] = int(k_value)
            elif memory_type == "ConversationTokenBufferMemory":
                memory_params["max_token_limit"] = int(token_limit)
            elif memory_type == "ConversationSummary":
                memory_params["max_token_limit"] = int(summary_token_limit)
            elif memory_type == "VectorStoreRetrieverMemory":
                memory_params["k"] = int(vector_k)

            # ì„¤ì • ì—…ë°ì´íŠ¸
            result = app.update_settings(
                model_name, temperature, memory_type, memory_params
            )
            return result

        # ë©”ì‹œì§€ ì „ì†¡ ì´ë²¤íŠ¸
        msg.submit(user_submit, [msg, chatbot], [msg, chatbot]).then(
            bot_respond, chatbot, [chatbot, vector_search_output]
        )
        submit.click(user_submit, [msg, chatbot], [msg, chatbot]).then(
            bot_respond, chatbot, [chatbot, vector_search_output]
        )

        # ëŒ€í™” ì´ˆê¸°í™”
        def clear_conversation():
            """ëŒ€í™” ë‚´ì—­ê³¼ ë©”ëª¨ë¦¬ ëª¨ë‘ ì´ˆê¸°í™”"""
            app.clear_conversation()  # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
            return [], "", ""  # ì±„íŒ…ë´‡ í™”ë©´, ì…ë ¥ì°½, ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™”

        clear.click(clear_conversation, outputs=[chatbot, msg, vector_search_output])

        # ë©”ëª¨ë¦¬ íƒ€ì… ë³€ê²½ì‹œ UI ì—…ë°ì´íŠ¸
        memory_dropdown.change(
            update_memory_ui,
            inputs=[memory_dropdown],
            outputs=[
                window_params,
                token_params,
                summary_params,
                vector_params,
                vector_search_output,
            ],
        )

        # ì„¤ì • ì—…ë°ì´íŠ¸
        update_btn.click(
            apply_settings,
            inputs=[
                model_dropdown,
                temperature_slider,
                memory_dropdown,
                k_slider,
                token_slider,
                summary_token_slider,
                vector_k_slider,
            ],
            outputs=[settings_output],
        )

        # ì˜ˆì œ ì…ë ¥
        gr.Examples(
            examples=[
                "ì•ˆë…•í•˜ì„¸ìš”! ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.",
                "ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì´ê³ , 30ì‚´ ê°œë°œìì…ë‹ˆë‹¤.",
                "ë°©ê¸ˆ ì œê°€ ë§í•œ ì œ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³  ìˆë‚˜ìš”?",
                "íŒŒì´ì¬ìœ¼ë¡œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                "ì´ì „ì— ì„¤ëª…í•œ í”¼ë³´ë‚˜ì¹˜ ìˆ˜ì—´ê³¼ ê´€ë ¨í•´ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆì–´ìš”.",
                "ì²˜ìŒ ëŒ€í™” ë‚´ìš©ì„ ë‹¤ì‹œ ë§í•´ì£¼ì„¸ìš”.",
            ],
            inputs=msg,
        )

    return demo


if __name__ == "__main__":
    # Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    demo = create_gradio_interface()
    demo.launch(
        share=False,  # Trueë¡œ ì„¤ì •í•˜ë©´ ê³µê°œ URL ìƒì„±
        server_name="127.0.0.1",  # ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥
        server_port=7860,  # í¬íŠ¸ ì„¤ì •
        show_error=True,
    )
