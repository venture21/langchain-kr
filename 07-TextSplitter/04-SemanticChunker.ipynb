{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00f269d",
   "metadata": {},
   "source": [
    "# SemanticChunker\n",
    "\n",
    "텍스트를 의미론적 유사성에 기반하여 분할합니다.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
    "\n",
    "이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b48cce",
   "metadata": {},
   "source": [
    "샘플 텍스트를 로드하고 내용을 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c170dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처\n"
     ]
    }
   ],
   "source": [
    "# data/appendix-keywords.txt 파일을 열어서 f라는 파일 객체를 생성합니다.\n",
    "with open(\"./data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니다.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1817a14",
   "metadata": {},
   "source": [
    "## SemanticChunker 생성\n",
    "\n",
    "`SemanticChunker`는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.\n",
    "\n",
    "이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06b68b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33ae70",
   "metadata": {},
   "source": [
    "`SemanticChunker`를 사용하여 텍스트를 의미적으로 관련된 청크로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312e3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab515b0",
   "metadata": {},
   "source": [
    "## 텍스트 분할\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9b20b",
   "metadata": {},
   "source": [
    "- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb5870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a777bc",
   "metadata": {},
   "source": [
    "분할된 청크를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eec69bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 분할된 청크 중 첫 번째 청크를 출력합니다.\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03b26b",
   "metadata": {},
   "source": [
    "`create_documents()` 함수를 사용하여 청크를 문서로 변환할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fadaf823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "print(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1633cf8e",
   "metadata": {},
   "source": [
    "## Breakpoints\n",
    "\n",
    "이 chunker는 문장을 \"분리\"할 시점을 결정하여 작동합니다. 이는 두 문장 간의 임베딩 차이를 살펴봄으로써 이루어집니다.\n",
    "\n",
    "그 차이가 특정 임계값을 넘으면 문장이 분리됩니다.\n",
    "\n",
    "- 참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580\n",
    "\n",
    "### Percentile\n",
    "\n",
    "기본적인 분리 방식은 백분위수(`Percentile`) 를 기반으로 합니다.\n",
    "\n",
    "이 방법에서는 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "744bbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 유형을 백분위수로 설정합니다.\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=70,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa8318",
   "metadata": {},
   "source": [
    "분할된 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c7b3262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e83f74",
   "metadata": {},
   "source": [
    "`docs`의 길이를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c0cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))  # docs의 길이를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1c9e8",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "이 방법에서는 지정한 `breakpoint_threshold_amount` 표준편차보다 큰 차이가 있는 경우 분할됩니다.\n",
    "\n",
    "- `breakpoint_threshold_type` 매개변수를 \"standard_deviation\"으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16a8d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준으로 표준 편차를 사용합니다.\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690db96c",
   "metadata": {},
   "source": [
    "분할된 결과를 확인합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1764de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0743d8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095170af",
   "metadata": {},
   "source": [
    "`docs`의 길이를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee9f46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))  # docs의 길이를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b03d9b",
   "metadata": {},
   "source": [
    "### Interquartile\n",
    "\n",
    "이 방법에서는 사분위수 범위(interquartile range)를 사용하여 청크를 분할합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb408177",
   "metadata": {},
   "source": [
    "- `breakpoint_threshold_type` 매개변수를 \"interquartile\"로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f32f5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.\n",
    "    breakpoint_threshold_type=\"interquartile\",\n",
    "    breakpoint_threshold_amount=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12e0d2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "\n",
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "\n",
      "정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "Tokenizer\n",
      "\n",
      "정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n",
      "\n",
      "VectorStore\n",
      "\n",
      "정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n",
      "\n",
      "SQL\n",
      "\n",
      "정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n",
      "\n",
      "CSV\n",
      "\n",
      "정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# text_splitter를 사용하여 분할합니다.\n",
    "docs = text_splitter.create_documents([file])\n",
    "\n",
    "# 결과를 출력합니다.\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d186bb7",
   "metadata": {},
   "source": [
    "`docs`의 길이를 출력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c693c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))  # docs의 길이를 출력합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb1eb65",
   "metadata": {},
   "source": [
    "### 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9dec0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국의 한산한 어촌에서 세계로 뻗어나간 IT 전문가, 박민호의 여정\n",
      "\n",
      "박민호는 한국의 동해안에 위치한 작은 어촌마을에서 태어났다. 그 마을은 바다와 맞닿아 있어 매일 아침 갈매기 소리와 함께 하루가 시작되는 곳이었다. 어릴 적 박민호는 종종 아버지의 낡은 어선을 바라보며 꿈을 키웠다. 그 꿈은 바다를 넘어 더 넓은 세상으로 나아가는 것이었다.\n",
      "그는 어릴 때부터 탁월한 학습 능력을 보여주었다. 초등학교 시절부터 수학과 과학 분야에서 두각을 나타냈고, 중학교에 입학했을 때는 이미 고등학교 수준의 문제를 풀 수 있었다. 그의 재능은 곧 주변 사람들의 이목을 끌었고, 마을 사람들은 그를 가리켜 \"우리 마을의 자랑\"이라고 불렀다\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/sementic_example.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일의 내용을 읽어서 file 변수에 저장합니다.\n",
    "\n",
    "# 파일으로부터 읽은 내용을 일부 출력합니|다.\n",
    "print(file[:350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1377d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 유형을 백분위수로 설정합니다.\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 유형을 백분위수로 설정합니다.\n",
    "    breakpoint_threshold_type=\"standard_deviation\",\n",
    "    breakpoint_threshold_amount=1.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7e3c58aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.\n",
    "    OpenAIEmbeddings(),\n",
    "    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.\n",
    "    breakpoint_threshold_type=\"interquartile\",\n",
    "    breakpoint_threshold_amount=0.20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b5e001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "한국의 한산한 어촌에서 세계로 뻗어나간 IT 전문가, 박민호의 여정\n",
      "\n",
      "박민호는 한국의 동해안에 위치한 작은 어촌마을에서 태어났다. 그 마을은 바다와 맞닿아 있어 매일 아침 갈매기 소리와 함께 하루가 시작되는 곳이었다. 어릴 적 박민호는 종종 아버지의 낡은 어선을 바라보며 꿈을 키웠다. 그 꿈은 바다를 넘어 더 넓은 세상으로 나아가는 것이었다. 그는 어릴 때부터 탁월한 학습 능력을 보여주었다. 초등학교 시절부터 수학과 과학 분야에서 두각을 나타냈고, 중학교에 입학했을 때는 이미 고등학교 수준의 문제를 풀 수 있었다. 그의 재능은 곧 주변 사람들의 이목을 끌었고, 마을 사람들은 그를 가리켜 \"우리 마을의 자랑\"이라고 불렀다. 고등학교를 졸업하기 직전, 박민호는 전국에서 500명만이 받는다는 '대통령 과학 장학생'으로 선정되었다. 이 소식은 작은 어촌마을에 큰 파문을 일으켰다. 마을 사람들은 모두 그를 축하해 주었고, 어머니는 기쁨의 눈물을 흘렸다. 이 장학금은 1990년에 제정된 것으로, 과학 분야에서 뛰어난 성과를 보인 학생들에게 주어지는 상이었다. 박민호가 장학생으로 선정되었을 당시, 이 장학금은 정말 우수한 학생들에게만 주어졌다. 그러나 시간이 지나면서 영향력 있는 가정의 자녀들에게 주기 위해 부정이 개입되기도 했다는 소문이 돌았다. 이는 박민호에게 더 큰 자부심을 안겨주었다.\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "그는 자신의 능력으로 이 영예를 얻었다는 사실에 큰 긍지를 느꼈다. 이 상을 받은 학생들은 곧바로 대한민국 최고의 대학에 진학할 수 있었고, 이는 엄청난 특혜였다. 박민호 역시 서울대학교 컴퓨터공학과에 입학했다. 작은 어촌마을에서 자란 그에게 서울은 전혀 다른 세상이었다. 높은 빌딩들과 복잡한 지하철, 그리고 끊임없이 움직이는 사람들. 모든 것이 새롭고 놀라웠다. 서울대학교에서의 4년은 박민호에게 큰 도전이었다. 학업의 난이도는 그가 예상했던 것보다 훨씬 높았고, 대도시의 생활 역시 적응하기 쉽지 않았다. 그러나 그는 포기하지 않았다. 밤을 새워가며 공부했고, 모르는 것이 있으면 교수님들을 찾아가 끊임없이 질문했다. 그의 열정과 노력은 곧 결실을 맺었고, 그는 학과에서 상위권의 성적을 유지할 수 있었다. 현재 박민호는 미국 실리콘밸리의 한 유명 IT 기업에서 인공지능 연구원으로 일하고 있다. 그러나 서울대 졸업 후 여러 차례의 좌절과 실패를 겪으며 IT 업계에서 성공하기까지 그의 여정은 결코 순탄하지 않았다. 그 과정에는 수많은 도전과 좌절, 그리고 새로운 시작이 있었다. 어린 시절과 꿈\n",
      "\n",
      "박민호가 태어난 어촌마을은 오랜 역사를 지니고 있었다. 이 마을은 조선 시대부터 어업이 번성한 곳으로, 수백 년 동안 변함없이 바다를 삶의 터전으로 삼아온 사람들의 이야기가 깃든 곳이었다. 박민호는 1985년 이 마을에서 태어났다. 그의 아버지는 3대째 이어온 가업을 이어받아 어부로 일하며 생계를 이어갔다. 어린 시절 박민호는 종종 아버지와 함께 바다에 나갔다. 출렁이는 파도와 끝없이 펼쳐진 수평선을 바라보며, 그는 언젠가 이 바다를 넘어 더 넓은 세상으로 나아가고 싶다는 생각을 했다. 아버지는 그런 아들의 꿈을 알아챘는지, 박민호에게 자주 이런 말을 했다. \"민호야, 공부를 열심히 해서 어촌을 벗어나 큰 세상으로 나가야 한다.\" 아버지는 박민호에게 어촌의 한계를 벗어나 더 큰 꿈을 이루기를 바랐다. 그러나 13세 때 예기치 못한 비극이 찾아왔다. 아버지가 폭풍우 속에서 조업을 하다 바다에서 사고로 사망한 것이다. 이 소식을 들었을 때 박민호는 큰 충격에 빠졌다. 아버지의 장례식 날, 그는 밤새 울며 아버지와의 추억을 되새겼다. 특히 아버지가 늘 강조하던 '공부'에 대한 말씀이 계속해서 귓가에 맴돌았다. 한동안 방황하던 그는 결국 아버지의 유언을 마음에 새기고 공부에 전념하기로 결심했다. 그때부터 박민호의 생활은 180도 바뀌었다. 학교가 끝나면 곧장 도서관으로 향했고, 밤늦게까지 책과 씨름했다. 주말에도 그의 일과는 다르지 않았다. 친구들이 놀러 가자고 할 때마다 그는 공부를 핑계로 거절했다. 이런 노력 끝에 고등학교 졸업반이 되었을 때 그는 대통령 과학 장학생으로 선정될 만큼 뛰어난 성적을 거두었다. 그러나 기쁨도 잠시, 현실적인 문제가 그를 기다리고 있었다. 가족의 경제적 어려움 때문에 서울에서 대학을 다니는 것은 큰 부담이었다. 어머니는 눈물을 흘리며 민호에게 지역 대학을 가라고 권했다. 하지만 박민호는 포기하지 않았다. 그는 다양한 장학금을 알아보고 지원했으며, 아르바이트를 병행하며 학업을 이어갈 계획을 세웠다. 그의 의지를 본 어머니는 결국 그의 선택을 존중해주기로 했다. 어머니는 박민호에게 말했다. \"네 아버지가 살아계셨다면 얼마나 자랑스러워하셨을까.\" 이 말을 들은 박민호는 더욱 강한 의지를 가지고 서울로 향했다. 대학 시절과 군 생활\n",
      "서울대학교에 입학한 박민호는 4년 동안 컴퓨터 공학을 공부하며 여러 연구 프로젝트에 참여했다. 2000년대 초반 대한민국은 IT 붐이 일어나던 시기였다. 인터넷과 모바일 기술이 급속도로 발전하며 새로운 기회의 시대가 열리고 있었다. 박민호는 이런 시대의 흐름을 타고 더 큰 꿈을 꾸기 시작했다. 그러나 현실은 녹록지 않았다. 경제적 어려움과 여러 차례의 좌절을 겪으며 학업을 이어갔다. 대학 시절, 그는 공부 외에도 여러 어려움을 겪었다. 경제적으로 어려운 환경에서 학업을 이어가며, 각종 아르바이트를 병행했다. 그는 편의점, 식당, 심지어 연구 보조 아르바이트까지 하며 학비와 생활비를 마련했다. 때로는 너무 힘들어 포기하고 싶은 순간도 있었다.\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "하지만 그럴 때마다 아버지의 말씀을 떠올렸다. \"민호야, 넌 반드시 큰 사람이 될 거야.\" 이 말을 되새기며 그는 다시 일어설 수 있었다. 대학 3학년 때, 박민호는 인공지능에 대해 깊은 관심을 갖게 되었다. 그는 밤을 새워가며 관련 논문들을 읽었고, 교수님들을 찾아가 조언을 구했다. 그의 열정은 곧 결실을 맺어 학과 내 인공지능 연구 프로젝트에 참여할 기회를 얻었다. 이 경험은 그의 진로를 결정짓는 중요한 계기가 되었다. 군대에 가야 할 시기가 오자 박민호는 공군에 입대했다. 그는 자신의 전공을 살려 IT 관련 업무를 맡아 실무 경험을 쌓았다. 군대에서도 그는 끊임없이 공부하며 자기 계발을 멈추지 않았다. 부대 내 도서관에서 빌린 책들로 영어 공부를 했고, 틈틈이 프로그래밍 언어를 독학했다. 제대 후 복학한 박민호는 더욱 학업에 매진했다.\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "그의 끈기와 열정은 주위 사람들에게 큰 귀감이 되었다. 교수들은 그를 \"우리 학과의 보물\"이라고 불렀고, 후배들은 그에게 조언을 구하기 위해 줄을 섰다. 졸업할 때 그는 학과 수석의 영예를 안았다. 직업과 해외 진출 시도\n",
      "\n",
      "서울대를 졸업한 박민호는 큰 꿈을 안고 여러 IT 기업에 지원했다. 그는 자신의 능력을 인정받아 대기업에 입사할 수 있을 거라 믿었다.\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "그러나 현실은 냉혹했다. 연이은 탈락 통지에 그는 좌절감을 느꼈다. \"내가 부족한 걸까?\" 자신을 의심하기 시작했다. 결국 그는 작은 스타트업에서 일하기 시작했다. 급여는 많지 않았지만, 다양한 경험을 쌓을 수 있었다. 그는 이 시기를 자신의 실력을 갈고닦는 기회로 삼았다. 밤을 새워가며 새로운 기술을 익혔고, 회사 프로젝트에 혁신적인 아이디어를 제안했다. 2년이 지나자 그의 노력이 빛을 발하기 시작했다. 그가 개발한 알고리즘이 업계의 주목을 받게 된 것이다. 여러 기업에서 그에게 이직 제안을 해왔고, 그는 더 큰 회사로 옮길 수 있었다. 그러나 여전히 그의 꿈은 더 컸다. 그는 세계적인 IT 기업들이 모여 있는 미국 실리콘밸리로의 진출을 꿈꿨다. 수많은 지원서를 보냈지만, 대부분 거절당했다.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([file])\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
    "    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed1a348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'percentile' 방식으로 SemanticChunker를 설정합니다.\n",
      "\n",
      "총 5개의 청크로 분할되었습니다.\n",
      "------------------------------\n",
      "청크 1:\n",
      "\n",
      "인공지능(AI)은 현대 기술의 정점에 있으며, 기계가 인간처럼 학습하고, 추론하며, 문제를 해결할 수 있도록 만드는 광범위한 컴퓨터 과학 분야입니다. 그 핵심에는 데이터로부터 패턴을 인식하는 머신러닝과 인간의 뇌 신경망을 모방한 심층 신경망을 사용하는 딥러닝이 있습니다.\n",
      "\n",
      "------------------------------\n",
      "청크 2:\n",
      "한국 문화는 '한류(Hallyu)'라는 이름으로 전 세계적인 주목을 받고 있으며, 그 영향력은 날이 갈수록 커지고 있습니다.\n",
      "\n",
      "------------------------------\n",
      "청크 3:\n",
      "방탄소년단(BTS)과 블랙핑크(BLACKPINK)를 필두로 한 K팝은 글로벌 음악 시장의 주류로 자리 잡았습니다. 인체의 혈액 순환계는 심장을 중심으로 혈액과 혈관으로 이루어져 있으며, 생명 유지에 필수적인 역할을 수행합니다.\n",
      "\n",
      "------------------------------\n",
      "청크 4:\n",
      "혈액은 온몸의 세포에 산소와 영양분을 공급하고, 이산화탄소와 같은 노폐물을 수거하여 운반하는 중요한 기능을 합니다. 이러한 순환 과정은 동맥, 정맥, 모세혈관을 통해 끊임없이 이루어집니다.\n",
      "\n",
      "------------------------------\n",
      "청크 5:\n",
      "\n",
      "\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# --- 1. 원하는 chunker 유형을 여기서 선택하세요 ---\n",
    "# 옵션: 'percentile', 'standard_deviation', 'interquartile'\n",
    "chunker_type = \"percentile\"\n",
    "\n",
    "# --- 2. 임베딩 모델 및 텍스트 분할기 초기화 ---\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text_splitter = None  # 먼저 비워둠\n",
    "\n",
    "print(f\"'{chunker_type}' 방식으로 SemanticChunker를 설정합니다.\")\n",
    "\n",
    "if chunker_type == \"percentile\":\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"percentile\",\n",
    "        breakpoint_threshold_amount=35,\n",
    "    )\n",
    "elif chunker_type == \"standard_deviation\":\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"standard_deviation\",\n",
    "        breakpoint_threshold_amount=0.4,\n",
    "    )\n",
    "elif chunker_type == \"interquartile\":\n",
    "    text_splitter = SemanticChunker(\n",
    "        embeddings,\n",
    "        breakpoint_threshold_type=\"interquartile\",\n",
    "        breakpoint_threshold_amount=0.20,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"'{chunker_type}'은(는) 유효한 chunker 유형이 아닙니다. 'percentile', 'standard_deviation', 'interquartile' 중에서 선택해주세요.\"\n",
    "    )\n",
    "\n",
    "# --- 4. 테스트용 텍스트 및 분할 실행 ---\n",
    "sample_text = \"\"\"\n",
    "인공지능(AI)은 현대 기술의 정점에 있으며, 기계가 인간처럼 학습하고, 추론하며, 문제를 해결할 수 있도록 만드는 광범위한 컴퓨터 과학 분야입니다. 그 핵심에는 데이터로부터 패턴을 인식하는 머신러닝과 인간의 뇌 신경망을 모방한 심층 신경망을 사용하는 딥러닝이 있습니다.\n",
    "\n",
    "한국 문화는 '한류(Hallyu)'라는 이름으로 전 세계적인 주목을 받고 있으며, 그 영향력은 날이 갈수록 커지고 있습니다. 방탄소년단(BTS)과 블랙핑크(BLACKPINK)를 필두로 한 K팝은 글로벌 음악 시장의 주류로 자리 잡았습니다.\n",
    "\n",
    "인체의 혈액 순환계는 심장을 중심으로 혈액과 혈관으로 이루어져 있으며, 생명 유지에 필수적인 역할을 수행합니다. 혈액은 온몸의 세포에 산소와 영양분을 공급하고, 이산화탄소와 같은 노폐물을 수거하여 운반하는 중요한 기능을 합니다. 이러한 순환 과정은 동맥, 정맥, 모세혈관을 통해 끊임없이 이루어집니다.\n",
    "\"\"\"\n",
    "\n",
    "if text_splitter:\n",
    "    docs = text_splitter.create_documents([sample_text])\n",
    "    print(f\"\\n총 {len(docs)}개의 청크로 분할되었습니다.\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"청크 {i+1}:\\n{doc.page_content}\\n\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf50f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-hTuzgesB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
