{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 아이디를 입력합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Owner 지정\n",
    "PROMPT_OWNER = \"venture21\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약: Stuff Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Please summarize the sentence according to the following REQUEST.\\nREQUEST:\\n1. Summarize the main points in bullet points.\\n2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\\n3. Use various emojis to make the summary more interesting.\\n4. DO NOT include any unnecessary information.\\n\\nCONTEXT:\\n{context}\\n\\nSUMMARY:\"\\n')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_title = \"summary-stuff-documents\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt_template = \"\"\"Please summarize the sentence according to the following REQUEST.\n",
    "REQUEST:\n",
    "1. Summarize the main points in bullet points.\n",
    "2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
    "3. Use various emojis to make the summary more interesting.\n",
    "4. DO NOT include any unnecessary information.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "SUMMARY:\"\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/summary-stuff-documents1/ccf1dc3e?organizationId=60c19a2a-c263-565c-a56e-2e0da19570d0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['docs'], input_types={}, partial_variables={}, template='You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\\nPlease provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. \\nThe summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. \\nPlease ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. \\nThe length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\\n\\nGIVEN DOCUMENTS:\\n{docs}\\n\\nFORMAT:\\n1. main theme 1\\n2. main theme 2\\n3. main theme 3\\n...\\n\\nCAUTION:\\n- DO NOT list more than 5 main themes.\\n\\nHelpful Answer:\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_title = \"map-prompt1\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "# 당신은 주어진 문서에서 핵심 주제를 추출하는 데 도움을 주는 전문 언론인입니다.\n",
    "# 주어진 문서를 바탕으로 번호가 매겨진 목록 형식으로 포괄적인 요약본을 제공합니다.\n",
    "# 요약본은 원문이 제시하는 모든 핵심 요점과 주요 아이디어를 담아야 하며, 정보를 간결하고\n",
    "# 이해하기 쉬운 형식으로 압축해야 합니다.\n",
    "# 주요 아이디어를 뒷받침하는 관련 세부 사항과 예시를 반드시 포함하되,\n",
    "# 불필요한 정보나 반복적인 내용은 피해야 합니다.\n",
    "# 요약본의 길이는 원문의 길이와 복잡성에 적합해야 하며, 중요한 정보가 누락되지 않도록\n",
    "# 명확하고 정확한 개요를 제공해야 합니다.\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful expert journalist in extracting the main themes from a GIVEN DOCUMENTS below.\n",
    "Please provide a comprehensive summary of the GIVEN DOCUMENTS in numbered list format. \n",
    "The summary should cover all the key points and main ideas presented in the original text, while also condensing the information into a concise and easy-to-understand format. \n",
    "Please ensure that the summary includes relevant details and examples that support the main ideas, while avoiding any unnecessary information or repetition. \n",
    "The length of the summary should be appropriate for the length and complexity of the original text, providing a clear and accurate overview without omitting any important information.\n",
    "\n",
    "GIVEN DOCUMENTS:\n",
    "{docs}\n",
    "\n",
    "FORMAT:\n",
    "1. main theme 1\n",
    "2. main theme 2\n",
    "3. main theme 3\n",
    "...\n",
    "\n",
    "CAUTION:\n",
    "- DO NOT list more than 5 main themes.\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/map-prompt1/28669aaf?organizationId=60c19a2a-c263-565c-a56e-2e0da19570d0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_title = \"reduce-prompt\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt_template = \"\"\"You are a helpful expert in summary writing.\n",
    "You are given numbered lists of summaries.\n",
    "Extract top 10 most important insights from the summaries.\n",
    "Then, write a summary of the insights in KOREAN.\n",
    "\n",
    "LIST OF SUMMARIES:\n",
    "{doc_summaries}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_title = \"chain-of-density-reduce-korean\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt_template = \"\"\"You are a helpful expert in summary writing. You are given lists of summaries.\n",
    "Please sum up previously summarized sentences according to the following REQUEST.\n",
    "REQUEST:\n",
    "1. Summarize the main points in bullet points in KOREAN.\n",
    "2. Each summarized sentence must start with an emoji that fits the meaning of the each sentence.\n",
    "3. Use various emojis to make the summary more interesting.\n",
    "4. MOST IMPORTANT points should be organized at the top of the list.\n",
    "5. DO NOT include any unnecessary information.\n",
    "\n",
    "LIST OF SUMMARIES:\n",
    "{doc_summaries}\n",
    "\n",
    "Helpful Answer: \"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_title = \"metadata-tagger\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "# 아래 제공된 제품 후기를 바탕으로, 고객이 언급한 핵심 사항들을 종합적으로 분석해 주세요.\n",
    "# 특히 제품 디자인을 중심으로 긍정적인 측면과 개선이 필요한 부분을 구분하여 평가해 주시기 바랍니다.\n",
    "# 고객이 높이 평가하거나 강조한 제품의 주요 특징 또는 속성을 구체적으로 파악해 주세요.\n",
    "# 특히 키감, 키 소리, 전반적인 사용자 경험, 충전 방식, 제품 디자인 등과 관련된 언급을 찾아주세요.\n",
    "# 이러한 속성에 대해 표현된 감정을 기반으로 후기의 전반적인 분위기(긍정적, 중립적, 부정적)를 평가해 주세요.\n",
    "# 추가적으로, 고객이 만족한 긍정적인 디자인 측면을 상세히 평가하고, 언급된 개선 사항이나 아쉬운 점을 기록해 주세요.\n",
    "# 후기 시작 부분에 표시된 고객의 제품 평점(1~5점 척도)을 추출해 주세요.\n",
    "# 분석 결과를 구조화된 JSON 형식으로 요약해 주세요. 이 JSON에는 키워드 배열, 디자인 평가, 만족한 점,\n",
    "# 개선 영역, 평가된 분위기, 그리고 숫자 평점이 포함되어야 합니다.\n",
    "\n",
    "prompt_template = \"\"\"Given the following product review, conduct a comprehensive analysis to extract key aspects mentioned by the customer, with a focus on evaluating the product's design and distinguishing between positive aspects and areas for improvement. \n",
    "Identify primary features or attributes of the product that the customer appreciated or highlighted, specifically looking for mentions related to the feel of the keys, sound produced by the keys, overall user experience, charging aspect, and the design of the product, etc. \n",
    "Assess the overall tone of the review (positive, neutral, or negative) based on the sentiment expressed about these attributes. \n",
    "Additionally, provide a detailed evaluation of the design, outline the positive aspects that the customer enjoyed, and note any areas of improvement or disappointment mentioned. \n",
    "Extract the customer's rating of the product on a scale of 1 to 5, as indicated at the beginning of the review. \n",
    "Summarize your findings in a structured JSON format, including an array of keywords, evaluations for design, satisfaction points, improvement areas, the assessed tone, and the numerical rating. \n",
    "\n",
    "INPUT:\n",
    "{input}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Density 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_title = \"chain-of-density-korean\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 5 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (100 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "Use only KOREAN language to reply.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Density (Korean) - 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_title = \"chain-of-density-map-korean\"\n",
    "\n",
    "# 요약문을 작성하기 위한 프롬프트 정의 (직접 프롬프트를 작성하는 경우)\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 3 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (100 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer \"Missing Entities\" and \"Denser_Summary\" as in TEXT format.\n",
    "Use only KOREAN language to reply.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 문서 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_title = \"rag-korean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 답변은 한글로 답변해 주세요.\"\"\"\n",
    "\n",
    "human = \"\"\"#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, parent_commit_hash=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG + 출처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_title = \"rag-korean-with-source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요. 출처(page, source)를 답변에 포함하세요. 답변은 한글로 답변해 주세요.\"\"\"\n",
    "\n",
    "human = \"\"\"#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub.push(f\"{PROMPT_OWNER}/{prompt_title}\", prompt, parent_commit_hash=\"latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    # LLM 평가자(심사위원)로서, 주어진 질문에 대한 LLM의 답변을 평가해 주세요.\n",
    "    # 제공된 맥락(context)을 바탕으로 답변의 정확성(Accuracy), 포괄성(Comprehensiveness),\n",
    "    # 그리고 **맥락의 정밀성(Context Precision)**을 평가해 주세요.\n",
    "    # 평가 후, 다음 형식에 맞춰 점수만 반환해 주세요:\n",
    "    # Accuracy: [점수]\n",
    "    # Comprehensiveness: [점수]\n",
    "    # Context Precision: [점수]\n",
    "    # Final: [정규화된 점수]\n",
    "    # 평가 기준:\n",
    "    # 정확성 (0-10점):\n",
    "    # 제공된 맥락의 정보와 답변이 얼마나 잘 일치하는지 평가합니다.\n",
    "    # 0점: 답변이 완전히 부정확하거나 제공된 맥락과 모순됩니다.\n",
    "    # 4점: 답변이 맥락과 부분적으로 일치하지만, 심각한 부정확성을 포함합니다.\n",
    "    # 7점: 답변이 대부분 맥락과 일치하지만, 사소한 부정확성이나 누락이 있습니다.\n",
    "    # 10점: 답변이 제공된 맥락과 완전히 일치하며, 완벽하게 정확합니다.\n",
    "    # 포괄성 (0-10점):\n",
    "    # 0점: 답변이 완전히 부적절하거나 질문과 관련이 없습니다.\n",
    "    # 3점: 답변이 정확하지만, 질문을 완전히 다루기에는 너무 간략합니다.\n",
    "    # 7점: 답변이 주요 측면은 다루고 있지만, 세부 사항이 부족하거나 사소한 부분을 놓쳤습니다.\n",
    "    # 10점: 답변이 질문의 모든 측면을 포괄적으로 다룹니다.\n",
    "    # 맥락의 정밀성 (0-10점):\n",
    "    # 제공된 맥락의 정보를 얼마나 정밀하게 사용했는지 평가합니다.\n",
    "    # 0점: 답변이 맥락의 정보를 전혀 사용하지 않거나 완전히 잘못 사용했습니다.\n",
    "    # 4점: 답변이 맥락의 정보를 일부 사용했지만, 상당한 오해가 있습니다.\n",
    "    # 7점: 답변이 관련 맥락 정보를 대부분 올바르게 사용했지만, 사소한 오해가 있습니다.\n",
    "    # 10점: 답변이 맥락의 모든 관련 정보를 정확하고 정밀하게 사용했습니다.\n",
    "    # 최종 정규화된 점수:\n",
    "    # 정확성, 포괄성, 맥락의 정밀성 점수를 합산한 뒤, 30으로 나누어 0에서 1 사이의 점수를 계산합니다.\n",
    "    # 공식: (정확성 + 포괄성 + 맥락의 정밀성) / 30\n",
    "    \"\"\"\n",
    "As an LLM evaluator (judge), please assess the LLM's response to the given question. Evaluate the response's accuracy, comprehensiveness, and context precision based on the provided context. After your evaluation, return only the numerical scores in the following format:\n",
    "Accuracy: [score]\n",
    "Comprehensiveness: [score]\n",
    "Context Precision: [score]\n",
    "Final: [normalized score]\n",
    "Grading rubric:\n",
    "\n",
    "Accuracy (0-10 points):\n",
    "Evaluate how well the answer aligns with the information provided in the given context.\n",
    "\n",
    "0 points: The answer is completely inaccurate or contradicts the provided context\n",
    "4 points: The answer partially aligns with the context but contains significant inaccuracies\n",
    "7 points: The answer mostly aligns with the context but has minor inaccuracies or omissions\n",
    "10 points: The answer fully aligns with the provided context and is completely accurate\n",
    "\n",
    "\n",
    "Comprehensiveness (0-10 points):\n",
    "\n",
    "0 points: The answer is completely inadequate or irrelevant\n",
    "3 points: The answer is accurate but too brief to fully address the question\n",
    "7 points: The answer covers main aspects but lacks detail or misses minor points\n",
    "10 points: The answer comprehensively covers all aspects of the question\n",
    "\n",
    "\n",
    "Context Precision (0-10 points):\n",
    "Evaluate how precisely the answer uses the information from the provided context.\n",
    "\n",
    "0 points: The answer doesn't use any information from the context or uses it entirely incorrectly\n",
    "4 points: The answer uses some information from the context but with significant misinterpretations\n",
    "7 points: The answer uses most of the relevant context information correctly but with minor misinterpretations\n",
    "10 points: The answer precisely and correctly uses all relevant information from the context\n",
    "\n",
    "\n",
    "Final Normalized Score:\n",
    "Calculate by summing the scores for accuracy, comprehensiveness, and context precision, then dividing by 30 to get a score between 0 and 1.\n",
    "Formula: (Accuracy + Comprehensiveness + Context Precision) / 30\n",
    "\n",
    "#Given question:\n",
    "{question}\n",
    "\n",
    "#LLM's response:\n",
    "{answer}\n",
    "\n",
    "#Provided context:\n",
    "{context}\n",
    "\n",
    "Please evaluate the LLM's response according to the criteria above. \n",
    "\n",
    "In your output, include only the numerical scores for FINAL NORMALIZED SCORE without any additional explanation or reasoning.\n",
    "ex) 0.81\n",
    "\n",
    "#Final Normalized Score(Just the number):\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "hub.push(f\"teddynote/context-answer-evaluator\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic m   eaning related to the user question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hub.push(f\"teddynote/retrieval-question-grader\", grade_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to the answer. \\n \n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the answer, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the answer.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {context} \\n\\n Answer: {answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hub.push(f\"teddynote/retrieval-answer-grader\", grade_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-hTuzgesB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
